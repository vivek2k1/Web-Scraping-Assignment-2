{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da39509e",
   "metadata": {},
   "source": [
    "## Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. \n",
    "\n",
    "You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10\n",
    "jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Analyst” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6a40f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e6cf16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cunnecting the web Driver\n",
    "driver=webdriver.Chrome(r'F:\\chromedriver.exe')\n",
    "url='https://www.naukri.com/'\n",
    "driver.get(url)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8335b1df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"aaa555e7aaf9494bf3b9d00edade1320\", element=\"ac0298a5-32a2-4977-a396-b672ad959242\")>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find web element for search bar using class name\n",
    "search_job = driver.find_element(By.CLASS_NAME,'suggestor-input')\n",
    "search_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b60db357",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write on search bar\n",
    "search_job.send_keys(\"Data Analyst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bcd5d45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"aaa555e7aaf9494bf3b9d00edade1320\", element=\"89f05c04-df1f-4d41-9114-985b04df38f5\")>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding web element for search location bar using relative Xpath \n",
    "search_loc = driver.find_element(By.XPATH,\"/html/body/div[1]/div[2]/div[3]/div/div/div[5]/div/div/div/input\")\n",
    "search_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c51fbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding web element for job locatin bar\n",
    "search_loc.send_keys(\"Bangalore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24fcc798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"aaa555e7aaf9494bf3b9d00edade1320\", element=\"940610ef-8130-49f4-9831-8a8d8f7c83d4\")>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clicking using web absolute xpath function \n",
    "search_btn=driver.find_element(By.XPATH,\"/html/body/div[1]/div[2]/div[3]/div/div/div[6]\")\n",
    "search_btn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "857bf316",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b52937",
   "metadata": {},
   "source": [
    "## Extracting Job Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c78515c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract all the web element having job titles\n",
    "title_tags = driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "len(title_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "101eccbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now the job title is inside the web element , we will run a for loop to extract.\n",
    "job_titles = []\n",
    "\n",
    "for i in title_tags:\n",
    "    job_titles.append(i.text)\n",
    "len(job_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa96f31",
   "metadata": {},
   "source": [
    "## Extracting Company Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3433c8df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract all the web element having company\n",
    "company_tags = driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "len(company_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e08f9e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now the company name is inside the web element , we will run a for loop to extract.\n",
    "company_names = []\n",
    "\n",
    "for i in company_tags:\n",
    "    company_names.append(i.text)\n",
    "len(company_names)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d01ea2c",
   "metadata": {},
   "source": [
    "## extrating job location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "93f3078c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract all the web element having location\n",
    "loc_tags = driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "len(loc_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8fb349df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now the job location is inside the web element , we will run a for loop to extract.\n",
    "job_loc = []\n",
    "\n",
    "for i in loc_tags:\n",
    "    job_loc.append(i.text)\n",
    "len(job_loc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899c1eb9",
   "metadata": {},
   "source": [
    "## extracting experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "332d2507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract all the web element having experience\n",
    "exp_tags = driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]')\n",
    "len(exp_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "69021414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now the experience is inside the web element , we will run a for loop to extract.\n",
    "experience = []\n",
    "\n",
    "for i in exp_tags:\n",
    "    experience.append(i.text)\n",
    "len(experience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9eaa56aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20, 20, 20)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(job_titles),len(job_loc),len(company_names),len(experience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0447a58d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Job Location</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Company Names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sr.Business Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru, karnataka\\n(WFH during Co...</td>\n",
       "      <td>6-11 Yrs</td>\n",
       "      <td>Collabera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sr Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>5-8 Yrs</td>\n",
       "      <td>Thomson Reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Data Analysis Analyst</td>\n",
       "      <td>Bangalore/Bengaluru, Pune, Chennai</td>\n",
       "      <td>7-12 Yrs</td>\n",
       "      <td>Capco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>2-3 Yrs</td>\n",
       "      <td>Thomson Reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analyst/Senior Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "      <td>Meesho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Financial Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "      <td>Nuance India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Analyst II</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "      <td>Cerner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Senior Data Analyst II</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "      <td>Flipkart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Senior Data Management Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>4-7 Yrs</td>\n",
       "      <td>Wells Fargo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Analyst - CRM Platform</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai, Hyderabad/Secunde...</td>\n",
       "      <td>1-6 Yrs</td>\n",
       "      <td>Artech infosystem</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Job Title  \\\n",
       "0          Sr.Business Data Analyst   \n",
       "1                   Sr Data Analyst   \n",
       "2      Senior Data Analysis Analyst   \n",
       "3                      Data Analyst   \n",
       "4  Data Analyst/Senior Data Analyst   \n",
       "5            Financial Data Analyst   \n",
       "6                   Data Analyst II   \n",
       "7            Senior Data Analyst II   \n",
       "8    Senior Data Management Analyst   \n",
       "9       Data Analyst - CRM Platform   \n",
       "\n",
       "                                        Job Location Experience  \\\n",
       "0  Bangalore/Bengaluru, karnataka\\n(WFH during Co...   6-11 Yrs   \n",
       "1                                Bangalore/Bengaluru    5-8 Yrs   \n",
       "2                 Bangalore/Bengaluru, Pune, Chennai   7-12 Yrs   \n",
       "3                                Bangalore/Bengaluru    2-3 Yrs   \n",
       "4                                Bangalore/Bengaluru    3-6 Yrs   \n",
       "5                                Bangalore/Bengaluru    3-8 Yrs   \n",
       "6                                Bangalore/Bengaluru    2-4 Yrs   \n",
       "7                                Bangalore/Bengaluru    3-6 Yrs   \n",
       "8                                Bangalore/Bengaluru    4-7 Yrs   \n",
       "9  Bangalore/Bengaluru, Mumbai, Hyderabad/Secunde...    1-6 Yrs   \n",
       "\n",
       "       Company Names  \n",
       "0          Collabera  \n",
       "1    Thomson Reuters  \n",
       "2              Capco  \n",
       "3    Thomson Reuters  \n",
       "4             Meesho  \n",
       "5       Nuance India  \n",
       "6             Cerner  \n",
       "7           Flipkart  \n",
       "8        Wells Fargo  \n",
       "9  Artech infosystem  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs=pd.DataFrame()\n",
    "jobs['Job Title']=job_titles\n",
    "jobs['Job Location']=job_loc\n",
    "jobs['Experience']=experience\n",
    "jobs['Company Names']=company_names\n",
    "jobs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc0b515",
   "metadata": {},
   "source": [
    "## Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "42e75354",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cunnecting the web Driver\n",
    "driver=webdriver.Chrome(r'F:\\chromedriver.exe')\n",
    "url='https://www.naukri.com/'\n",
    "driver.get(url)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d60aa634",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"92c72687b3a5017611f922f4acfbf87d\", element=\"ade7a9e1-d004-4a53-94b7-58652d653469\")>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find web element for search bar using class name\n",
    "search_job = driver.find_element(By.CLASS_NAME,'suggestor-input')\n",
    "search_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0c6f4d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write on search bar\n",
    "search_job.send_keys(\"Data Scientist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7a87561f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"92c72687b3a5017611f922f4acfbf87d\", element=\"de0bb2b2-8df8-47b2-87c8-d1d5bc0d3d50\")>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding web element for search location bar using relative Xpath \n",
    "search_loc = driver.find_element(By.XPATH,\"/html/body/div[1]/div[2]/div[3]/div/div/div[5]/div/div/div/input\")\n",
    "search_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7ab715a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding web element for job locatin bar\n",
    "search_loc.send_keys(\"Bangalore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0e268c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"92c72687b3a5017611f922f4acfbf87d\", element=\"95743275-913a-4511-82c8-ea9ae35b0e25\")>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clicking using web absolute xpath function \n",
    "search_btn=driver.find_element(By.XPATH,\"/html/body/div[1]/div[2]/div[3]/div/div/div[6]\")\n",
    "search_btn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "45d5f55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7a0c64",
   "metadata": {},
   "source": [
    "### Extracting Job Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "653c4e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract all the web element having job titles\n",
    "title_tags = driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "len(title_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "81da245c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now the job title is inside the web element , we will run a for loop to extract.\n",
    "job_titles = []\n",
    "\n",
    "for i in title_tags:\n",
    "    job_titles.append(i.text)\n",
    "len(job_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6180307",
   "metadata": {},
   "source": [
    "## extrating job location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "82f5e81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract all the web element having location\n",
    "loc_tags = driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "len(loc_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d23db1e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now the job location is inside the web element , we will run a for loop to extract.\n",
    "job_loc = []\n",
    "\n",
    "for i in loc_tags:\n",
    "    job_loc.append(i.text)\n",
    "len(job_loc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219d5751",
   "metadata": {},
   "source": [
    "## Extracting Company Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b47f81aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract all the web element having company\n",
    "company_tags = driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "len(company_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f3bee3b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now the company name is inside the web element , we will run a for loop to extract.\n",
    "company_names = []\n",
    "\n",
    "for i in company_tags:\n",
    "    company_names.append(i.text)\n",
    "len(company_names)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3f1f6eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Job Location</th>\n",
       "      <th>Company Names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist/ Senior Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru, Pune, Gurgaon/Gurugram, C...</td>\n",
       "      <td>Fractal Analytics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Urgent Job Opening For AI Practitioner - Data ...</td>\n",
       "      <td>Bangalore/Bengaluru, Kochi/Cochin, New Delhi, ...</td>\n",
       "      <td>Wipro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dataiku Consultant</td>\n",
       "      <td>Bangalore/Bengaluru, Pune, Chennai</td>\n",
       "      <td>Wipro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist: Advanced Analytics</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>IBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Research Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>IBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Science - Senior Data Scientist - Analytics</td>\n",
       "      <td>Bangalore/Bengaluru, Noida</td>\n",
       "      <td>Paytm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Principal - Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Schneider Electric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Research and Development -AI/ML -(PhD )</td>\n",
       "      <td>Bangalore/Bengaluru, Noida, Hyderabad/Secunder...</td>\n",
       "      <td>EXL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Opportunity For Data Scientist - Female Candid...</td>\n",
       "      <td>Bangalore/Bengaluru, Gurgaon/Gurugram, Mumbai ...</td>\n",
       "      <td>PayU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Senior Data Science Engineer</td>\n",
       "      <td>Bangalore/Bengaluru, Gurgaon/Gurugram, Mumbai ...</td>\n",
       "      <td>Fractal Analytics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job Title  \\\n",
       "0              Data Scientist/ Senior Data Scientist   \n",
       "1  Urgent Job Opening For AI Practitioner - Data ...   \n",
       "2                                 Dataiku Consultant   \n",
       "3                 Data Scientist: Advanced Analytics   \n",
       "4                                 Research Scientist   \n",
       "5   Data Science - Senior Data Scientist - Analytics   \n",
       "6                         Principal - Data Scientist   \n",
       "7            Research and Development -AI/ML -(PhD )   \n",
       "8  Opportunity For Data Scientist - Female Candid...   \n",
       "9                       Senior Data Science Engineer   \n",
       "\n",
       "                                        Job Location       Company Names  \n",
       "0  Bangalore/Bengaluru, Pune, Gurgaon/Gurugram, C...   Fractal Analytics  \n",
       "1  Bangalore/Bengaluru, Kochi/Cochin, New Delhi, ...               Wipro  \n",
       "2                 Bangalore/Bengaluru, Pune, Chennai               Wipro  \n",
       "3                                Bangalore/Bengaluru                 IBM  \n",
       "4                                Bangalore/Bengaluru                 IBM  \n",
       "5                         Bangalore/Bengaluru, Noida               Paytm  \n",
       "6                                Bangalore/Bengaluru  Schneider Electric  \n",
       "7  Bangalore/Bengaluru, Noida, Hyderabad/Secunder...                 EXL  \n",
       "8  Bangalore/Bengaluru, Gurgaon/Gurugram, Mumbai ...                PayU  \n",
       "9  Bangalore/Bengaluru, Gurgaon/Gurugram, Mumbai ...   Fractal Analytics  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs=pd.DataFrame()\n",
    "jobs['Job Title']=job_titles\n",
    "jobs['Job Location']=job_loc\n",
    "jobs['Company Names']=company_names\n",
    "jobs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161791dd",
   "metadata": {},
   "source": [
    "## Q3: In this question you have to scrape data using the filters available on the webpage as shown below:\n",
    "1. You have to use the location and salary filter.\n",
    "2. You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "3. You have to scrape the job-title, job-location, company name, experience required.\n",
    "4. The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "d83c5a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cunnecting the web Driver\n",
    "driver=webdriver.Chrome(r'F:\\chromedriver.exe')\n",
    "url='https://www.naukri.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "4e994b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"69a1bac879d7a69395113f5626d62fe5\", element=\"6ebfe050-6e71-4152-9250-4e8b632d371f\")>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find web element for search bar using class name\n",
    "search_job = driver.find_element(By.CLASS_NAME,'suggestor-input')\n",
    "search_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "40088776",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write on search bar\n",
    "search_job.send_keys(\"Data Scientist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "da3692d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"69a1bac879d7a69395113f5626d62fe5\", element=\"2b958bb1-f8b8-4d15-a195-276160ab914f\")>"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clicking using web absolute xpath function \n",
    "search_btn=driver.find_element(By.XPATH,\"/html/body/div[1]/div[2]/div[3]/div/div/div[6]\")\n",
    "search_btn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "943744d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking on search botton\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "87ee4353",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the salary range \n",
    "salary_check=driver.find_element(By.XPATH,\"/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[6]/div[2]/div[2]/label/i\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "2df0748a",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_check.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "6e65f2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the Location range \n",
    "location_check=driver.find_element(By.XPATH,\"/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[6]/div[2]/div[3]/label/i\")\n",
    "location_check.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7413068d",
   "metadata": {},
   "source": [
    "### Extracting Job Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "cfbbb0f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract all the web element having job titles\n",
    "title_tags = driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "len(title_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "b018968e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now the job title is inside the web element , we will run a for loop to extract.\n",
    "job_titles = []\n",
    "\n",
    "for i in title_tags:\n",
    "    job_titles.append(i.text)\n",
    "len(job_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a612249f",
   "metadata": {},
   "source": [
    "## extrating job location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "f643dd6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract all the web element having location\n",
    "loc_tags = driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "len(loc_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "1191b38f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now the job location is inside the web element , we will run a for loop to extract.\n",
    "job_loc = []\n",
    "\n",
    "for i in loc_tags:\n",
    "    job_loc.append(i.text)\n",
    "len(job_loc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03e36bd",
   "metadata": {},
   "source": [
    "## Extracting Company Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "ba7fcc5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract all the web element having company\n",
    "company_tags = driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "len(company_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "230ae63d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now the company name is inside the web element , we will run a for loop to extract.\n",
    "company_names = []\n",
    "\n",
    "for i in company_tags:\n",
    "    company_names.append(i.text)\n",
    "len(company_names)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670ee1f2",
   "metadata": {},
   "source": [
    "## extracting experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "0e21f6ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract all the web element having experience\n",
    "exp_tags = driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]')\n",
    "len(exp_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "86e2382a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now the experience is inside the web element , we will run a for loop to extract.\n",
    "experience = []\n",
    "\n",
    "for i in exp_tags:\n",
    "    experience.append(i.text)\n",
    "len(experience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "7e68e6d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Job Location</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Company Names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DigitalBCG GAMMA Data Scientist</td>\n",
       "      <td>New Delhi, Bangalore/Bengaluru</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "      <td>Boston Consulting Group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Associate - Data Science</td>\n",
       "      <td>Mumbai, Hyderabad/Secunderabad, Gurgaon/Gurugr...</td>\n",
       "      <td>4-7 Yrs</td>\n",
       "      <td>Black Turtle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist - Noida/Bangalore</td>\n",
       "      <td>Noida, Bangalore/Bengaluru</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "      <td>EXL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist - Machine learning AI</td>\n",
       "      <td>Delhi / NCR, Bangalore/Bengaluru, Mumbai (All ...</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "      <td>Teq Analytics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist - MIND Infotech</td>\n",
       "      <td>Noida</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "      <td>MOTHERSONSUMI INFOTECH &amp; DESIGNS LIMITED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist - Engine Algorithm</td>\n",
       "      <td>Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "      <td>Primo Hiring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Delhi / NCR, Pune, Bangalore/Bengaluru</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "      <td>Mount Talent Consulting Private Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Delhi / NCR, Pune, Bangalore/Bengaluru</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "      <td>Mount Talent Consulting Private Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Delhi / NCR, Bangalore/Bengaluru, Mumbai (All ...</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "      <td>Sydata Consulting India Pvt Ltd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist - Internet Jobs - II</td>\n",
       "      <td>Delhi / NCR, Bangalore/Bengaluru, Mumbai (All ...</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "      <td>Jobs Territory</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Job Title  \\\n",
       "0       DigitalBCG GAMMA Data Scientist   \n",
       "1       Senior Associate - Data Science   \n",
       "2      Data Scientist - Noida/Bangalore   \n",
       "3  Data Scientist - Machine learning AI   \n",
       "4        Data Scientist - MIND Infotech   \n",
       "5     Data Scientist - Engine Algorithm   \n",
       "6                        Data Scientist   \n",
       "7                        Data Scientist   \n",
       "8                        Data Scientist   \n",
       "9   Data Scientist - Internet Jobs - II   \n",
       "\n",
       "                                        Job Location Experience  \\\n",
       "0                     New Delhi, Bangalore/Bengaluru    2-5 Yrs   \n",
       "1  Mumbai, Hyderabad/Secunderabad, Gurgaon/Gurugr...    4-7 Yrs   \n",
       "2                         Noida, Bangalore/Bengaluru   5-10 Yrs   \n",
       "3  Delhi / NCR, Bangalore/Bengaluru, Mumbai (All ...    3-8 Yrs   \n",
       "4                                              Noida    4-8 Yrs   \n",
       "5  Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...    1-3 Yrs   \n",
       "6             Delhi / NCR, Pune, Bangalore/Bengaluru    2-4 Yrs   \n",
       "7             Delhi / NCR, Pune, Bangalore/Bengaluru    2-4 Yrs   \n",
       "8  Delhi / NCR, Bangalore/Bengaluru, Mumbai (All ...    3-8 Yrs   \n",
       "9  Delhi / NCR, Bangalore/Bengaluru, Mumbai (All ...    3-6 Yrs   \n",
       "\n",
       "                              Company Names  \n",
       "0                   Boston Consulting Group  \n",
       "1                              Black Turtle  \n",
       "2                                       EXL  \n",
       "3                             Teq Analytics  \n",
       "4  MOTHERSONSUMI INFOTECH & DESIGNS LIMITED  \n",
       "5                              Primo Hiring  \n",
       "6   Mount Talent Consulting Private Limited  \n",
       "7   Mount Talent Consulting Private Limited  \n",
       "8           Sydata Consulting India Pvt Ltd  \n",
       "9                            Jobs Territory  "
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs=pd.DataFrame()\n",
    "jobs['Job Title']=job_titles\n",
    "jobs['Job Location']=job_loc\n",
    "jobs['Experience']=experience\n",
    "jobs['Company Names']=company_names\n",
    "jobs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44dab30",
   "metadata": {},
   "source": [
    "## Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "4. Discount\n",
    "The attributes which you have to scrape is ticked marked in the below image.\n",
    "\n",
    "\n",
    "To scrape the data you have to go through following steps:\n",
    "1. Go to Flipkart webpage by url : https://www.flipkart.com/\n",
    "2. Enter “sunglasses” in the search field where “search for products, brands and more” is written and\n",
    "click the search icon\n",
    "3. After that you will reach to the page having a lot of sunglasses. From this page you can scrap the\n",
    "required data as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "e685bf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cunnecting the web Driver\n",
    "driver=webdriver.Chrome(r'F:\\chromedriver.exe')\n",
    "url='https://www.flipkart.com/search?q=sunglasses&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off'\n",
    "driver.get(url)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "a4df5cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"363e53b0bb9aee43f05b010b1e6b8d28\", element=\"79c1d588-1e69-4757-abb2-ae276bdfb2c0\")>"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find web element for search bar using class name\n",
    "search_bar = driver.find_element(By.CLASS_NAME,'_3704LK')\n",
    "search_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "248dcfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write on search bar\n",
    "search_bar.send_keys(\"sunglasses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "2a7a86a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"363e53b0bb9aee43f05b010b1e6b8d28\", element=\"8106d680-692d-4e82-af42-ce9cfa377a3e\")>"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clicking using web absolute xpath function \n",
    "search_btn=driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/button\")\n",
    "search_btn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "f3100e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking on search botton\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "15199d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_url=[]\n",
    "\n",
    "url=driver.find_elements(By.XPATH,\"//nav[@class='yFHi8N']//a\")\n",
    "for i in url:\n",
    "    page_url.append(i.get_attribute('href'))\n",
    "page_url=page_url[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "e2b73236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty list to scraping data \n",
    "\n",
    "brand=[]\n",
    "Description=[]\n",
    "Price=[]\n",
    "Discount=[] \n",
    "\n",
    "#Brand\n",
    "for i in page_url:\n",
    "    driver.get(i)\n",
    "    titles_brand=driver.find_elements(By.XPATH,\"//div[@class='_2WkVRV']\")\n",
    "    for i in titles_brand:\n",
    "        brand.append(i.text)\n",
    "        \n",
    "#Prduct Discription        \n",
    "    titles_Description=driver.find_elements(By.XPATH,\"//a[@class='IRpwTa']\")\n",
    "    for i in titles_Description:\n",
    "        Description.append(i.text)\n",
    "\n",
    "#Price\n",
    "    titles_Price=driver.find_elements(By.XPATH,\"//div[@class='_30jeq3']\")\n",
    "    for i in titles_Price:\n",
    "        Price.append(i.text)\n",
    "        \n",
    "#Discount    \n",
    "    titles_Discount=driver.find_elements(By.XPATH,\"//div[@class='_3Ay6Sb']/span\")\n",
    "    for i in titles_Discount:\n",
    "        Discount.append(i.text.replace(\"% off\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "64432a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product Description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DAHAAZIL</td>\n",
       "      <td>UV Protection, Night Vision, Riding Glasses Wa...</td>\n",
       "      <td>₹177</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>₹639</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New Specs</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>₹264</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>by Lenskart Polarized, UV Protection Rectangul...</td>\n",
       "      <td>₹749</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection Round Sunglasses (54)</td>\n",
       "      <td>₹315</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection, Gradient Retro Square Sunglasse...</td>\n",
       "      <td>₹474</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>Polarized, UV Protection Round Sunglasses (51)</td>\n",
       "      <td>₹1,099</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Urbanic</td>\n",
       "      <td>Others Retro Square Sunglasses (Free Size)</td>\n",
       "      <td>₹399</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>by Lenskart Polarized, UV Protection Cat-eye S...</td>\n",
       "      <td>₹799</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>elegante</td>\n",
       "      <td>Polarized, Riding Glasses, Night Vision Sports...</td>\n",
       "      <td>₹499</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Brand                                Product Description   Price  \\\n",
       "0         DAHAAZIL  UV Protection, Night Vision, Riding Glasses Wa...    ₹177   \n",
       "1         Fastrack   UV Protection Rectangular Sunglasses (Free Size)    ₹639   \n",
       "2        New Specs   UV Protection Rectangular Sunglasses (Free Size)    ₹264   \n",
       "3    VINCENT CHASE  by Lenskart Polarized, UV Protection Rectangul...    ₹749   \n",
       "4        Elligator                UV Protection Round Sunglasses (54)    ₹315   \n",
       "..             ...                                                ...     ...   \n",
       "95  ROZZETTA CRAFT  UV Protection, Gradient Retro Square Sunglasse...    ₹474   \n",
       "96   VINCENT CHASE     Polarized, UV Protection Round Sunglasses (51)  ₹1,099   \n",
       "97         Urbanic         Others Retro Square Sunglasses (Free Size)    ₹399   \n",
       "98   VINCENT CHASE  by Lenskart Polarized, UV Protection Cat-eye S...    ₹799   \n",
       "99        elegante  Polarized, Riding Glasses, Night Vision Sports...    ₹499   \n",
       "\n",
       "   Discount %  \n",
       "0          82  \n",
       "1          20  \n",
       "2          89  \n",
       "3          62  \n",
       "4          87  \n",
       "..        ...  \n",
       "95         76  \n",
       "96         45  \n",
       "97         49  \n",
       "98         60  \n",
       "99         78  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating dataframe for the first 100 sunglasses \n",
    "\n",
    "flipkart=pd.DataFrame({})\n",
    "flipkart['Brand']=brand[0:100]\n",
    "flipkart['Product Description']=Description[0:100]\n",
    "flipkart['Price']=Price[0:100]\n",
    "flipkart['Discount %']=Discount[0:100]\n",
    "\n",
    "flipkart\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a104534a",
   "metadata": {},
   "source": [
    "## Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.flipkart.com/\n",
    "2. Enter “iphone 11” in “Search” field .\n",
    "3. Then click the search button.\n",
    "You will reach to the below shown webpage .\n",
    "As shown in the above page you have to scrape the tick marked attributes.These are:\n",
    "1. Rating\n",
    "2. Review summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100 reviews.\n",
    "Note: All the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5da0166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the webpage through our web driver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "driver.get(\"https://www.flipkart.com/\")\n",
    "b=driver.find_element(By.XPATH,'/html/body/div[2]/div/div/button')\n",
    "b.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ea75b5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click on search button\n",
    "search_button=driver.find_element(By.CLASS_NAME,\"L0Z3Pu\")\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4436ac58",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.flipkart.com/apple-iphone-11-white-64-gb/p/itmfc6a7091eb20b?pid=MOBFWQ6BVWVEH3XE&lid=LSTMOBFWQ6BVWVEH3XEB1SFMZ&marketplace=FLIPKART&q=iphone+11&store=tyy%2F4io&srno=s_1_1&otracker=search&otracker1=search&fm=organic&iid=ffcd465c-1c35-4a3b-8c2b-8d0bc138972c.MOBFWQ6BVWVEH3XE.SEARCH&ppt=hp&ppn=homepage&ssid=x6aouugn8w0000001654762626827&qH=f6cdfdaa9f3c23f3\")\n",
    "phone=driver.find_element(By.XPATH,'/html/body/div[1]/div/div[3]/div[1]/div[2]/div[9]/div[6]/div/a/div')\n",
    "phone.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c8b7b4c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating empty list to scraping data \n",
    "rating=[]\n",
    "review_sum=[]\n",
    "review_dis=[]\n",
    "j=1\n",
    "# extracting all the tags\n",
    "def i_scrap():\n",
    "    rate=driver.find_elements(By.XPATH,'//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "    for i in rate:\n",
    "        rating.append(i.text)\n",
    "    review=driver.find_elements(By.XPATH,'//p[@class=\"_2-N8zT\"]')\n",
    "    for i in review:\n",
    "        review_sum.append(i.text)\n",
    "    rdis=driver.find_elements(By.XPATH,'//div[@class=\"t-ZTKy\"]')\n",
    "    for i in rdis:\n",
    "        review_dis.append(i.text)\n",
    "  #scraping i\n",
    "next_b=driver.find_elements(By.CLASS_NAME,\"ge-49M\")\n",
    "for b in next_b:\n",
    "    i_scrap()\n",
    "len(rating)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "496a6d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "      <th>Discription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Simply awesome</td>\n",
       "      <td>Really satisfied with the Product I received.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Amazing phone with great cameras and better ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Best in the market!</td>\n",
       "      <td>Great iPhone very snappy experience as apple k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Highly recommended</td>\n",
       "      <td>What a camera .....just awesome ..you can feel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Previously I was using one plus 3t it was a gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5</td>\n",
       "      <td>Fabulous!</td>\n",
       "      <td>This is my first iOS phone. I am very happy wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product</td>\n",
       "      <td>Amazing Powerful and Durable Gadget.\\n\\nI’m am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>i11 is worthy to buy, too much happy with the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>4</td>\n",
       "      <td>Good choice</td>\n",
       "      <td>So far it’s been an AMAZING experience coming ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5</td>\n",
       "      <td>Highly recommended</td>\n",
       "      <td>iphone 11 is a very good phone to buy only if ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating               Review  \\\n",
       "0       5       Simply awesome   \n",
       "1       5     Perfect product!   \n",
       "2       5  Best in the market!   \n",
       "3       5   Highly recommended   \n",
       "4       5    Worth every penny   \n",
       "..    ...                  ...   \n",
       "95      5            Fabulous!   \n",
       "96      5        Great product   \n",
       "97      5    Worth every penny   \n",
       "98      4          Good choice   \n",
       "99      5   Highly recommended   \n",
       "\n",
       "                                          Discription  \n",
       "0   Really satisfied with the Product I received.....  \n",
       "1   Amazing phone with great cameras and better ba...  \n",
       "2   Great iPhone very snappy experience as apple k...  \n",
       "3   What a camera .....just awesome ..you can feel...  \n",
       "4   Previously I was using one plus 3t it was a gr...  \n",
       "..                                                ...  \n",
       "95  This is my first iOS phone. I am very happy wi...  \n",
       "96  Amazing Powerful and Durable Gadget.\\n\\nI’m am...  \n",
       "97  i11 is worthy to buy, too much happy with the ...  \n",
       "98  So far it’s been an AMAZING experience coming ...  \n",
       "99  iphone 11 is a very good phone to buy only if ...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating dataframe for the first 100 reviews data from flipkart.com for iphone11 phone:\n",
    "iphone_11=pd.DataFrame({'Rating':rating,'Review':review_sum,'Discription':review_dis})\n",
    "iphone_11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d85a0b1",
   "metadata": {},
   "source": [
    "## Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the\n",
    "search field.\n",
    "You have to scrape 4 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "As shown in the below image, you have to scrape the tick marked attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "d3b06e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "\n",
    "driver.get(\"https://www.flipkart.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "6cd2c6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter “sneakers” in “search for products, brands and more” field.\n",
    "search_product = driver.find_element(By.XPATH,\"//div[@class='_3OO5Xc']/input\")\n",
    "search_product.send_keys(\"sneakers\")\n",
    "# click the search button\n",
    "search_btn = driver.find_element(By.XPATH,\"//button[@class='L0Z3Pu']\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "68afaee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying the url of the webpage to be scraped\n",
    "url = \"https://www.flipkart.com/search?q=sneakers&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off\"\n",
    "\n",
    "# open the webpage through our web driver\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "2701e654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating first 3 website pages to scrape the data\n",
    "\n",
    "page_url=[]\n",
    "\n",
    "url=driver.find_elements(By.XPATH,\"//nav[@class='yFHi8N']//a\")\n",
    "for i in url:\n",
    "    page_url.append(i.get_attribute('href'))\n",
    "page_url=page_url[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "0d4c72b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty list to scraping data into them\n",
    "\n",
    "brand=[]\n",
    "Description=[]\n",
    "Price=[]\n",
    "Discount=[] \n",
    "\n",
    "# extracting all the tags\n",
    "\n",
    "for i in page_url:\n",
    "    driver.get(i)\n",
    "    titles_brand=driver.find_elements(By.XPATH,\"//div[@class='_2WkVRV']\")\n",
    "    for i in titles_brand:\n",
    "        brand.append(i.text)\n",
    "        \n",
    "    titles_Description=driver.find_elements(By.XPATH,\"//a[@class='IRpwTa']\")\n",
    "    for i in titles_Description:\n",
    "        Description.append(i.text)\n",
    "        \n",
    "    titles_Price=driver.find_elements(By.XPATH,\"//div[@class='_30jeq3']\")\n",
    "    for i in titles_Price:\n",
    "        Price.append(i.text.replace(\"₹\",\"\"))\n",
    "        \n",
    "    titles_Discount=driver.find_elements(By.XPATH,\"//div[@class='_3Ay6Sb']/span\")\n",
    "    for i in titles_Discount:\n",
    "        Discount.append(i.text.replace(\"% off\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "8ac315ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 sneakers listings on flipkart.com:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product Description</th>\n",
       "      <th>Price₹</th>\n",
       "      <th>Discount %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LE GREEM</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>499</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LOUIS PHILIPPE</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>2,399</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WOODLAND</td>\n",
       "      <td>Modern Trendy Sneakers Shoes Sneakers For Men</td>\n",
       "      <td>2,096</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BRUTON</td>\n",
       "      <td>Modern Trendy Sneakers Shoes Sneakers For Men</td>\n",
       "      <td>269</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BRUTON</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>299</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>HOTSTYLE</td>\n",
       "      <td>Men's Sneakers Fashion Lightweight Running Sho...</td>\n",
       "      <td>271</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>LE GREEM</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>499</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>KNIGHT WALKERS</td>\n",
       "      <td>Stylish White Casual Walking Sneakers Shoes Fo...</td>\n",
       "      <td>648</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Layasa</td>\n",
       "      <td>Canvas Casual Partywear Outdoor Sneakers Shoes...</td>\n",
       "      <td>499</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>ASIAN</td>\n",
       "      <td>X-Ray 2 Square Sneakers For Men</td>\n",
       "      <td>498</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Brand                                Product Description Price₹  \\\n",
       "0         LE GREEM                                   Sneakers For Men    499   \n",
       "1   LOUIS PHILIPPE                                   Sneakers For Men  2,399   \n",
       "2         WOODLAND      Modern Trendy Sneakers Shoes Sneakers For Men  2,096   \n",
       "3           BRUTON      Modern Trendy Sneakers Shoes Sneakers For Men    269   \n",
       "4           BRUTON                                   Sneakers For Men    299   \n",
       "..             ...                                                ...    ...   \n",
       "95        HOTSTYLE  Men's Sneakers Fashion Lightweight Running Sho...    271   \n",
       "96        LE GREEM                                   Sneakers For Men    499   \n",
       "97  KNIGHT WALKERS  Stylish White Casual Walking Sneakers Shoes Fo...    648   \n",
       "98          Layasa  Canvas Casual Partywear Outdoor Sneakers Shoes...    499   \n",
       "99           ASIAN                    X-Ray 2 Square Sneakers For Men    498   \n",
       "\n",
       "   Discount %  \n",
       "0          50  \n",
       "1          40  \n",
       "2          30  \n",
       "3          79  \n",
       "4          76  \n",
       "..        ...  \n",
       "95         45  \n",
       "96         50  \n",
       "97         56  \n",
       "98         50  \n",
       "99         37  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating dataframe for the first 100 sneakers:\n",
    "\n",
    "flipkart=pd.DataFrame({})\n",
    "flipkart['Brand']=brand[0:100]\n",
    "flipkart['Product Description']=Description[0:100]\n",
    "flipkart['Price₹']=Price[0:100]\n",
    "flipkart['Discount %']=Discount[0:100]\n",
    "\n",
    "print(\"Top 100 sneakers listings on flipkart.com:\\n\")\n",
    "flipkart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0845df",
   "metadata": {},
   "source": [
    "## Q7: Go to the link - https://www.myntra.com/shoes\n",
    "Set second Price filter and Color filter to “Black”, as shown in the below image.\n",
    "And then scrape First 100 shoes data you get. The data should include “Brand” of the shoes , Short Shoe\n",
    "description, price of the shoe as shown in the below image.\n",
    "\n",
    "Note: Applying the filter and scraping the data, everything should be done through code only and there\n",
    "should not be any manual step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "358d6582",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "\n",
    "driver.get(\"https://www.myntra.com/shoes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab48de5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the Price filter to “Rs. 7149 to Rs. 14099” by checking the respective boxe\n",
    "\n",
    "price = driver.find_elements(By.XPATH,\"//ul[@class ='price-list']//label[@class ='common-customCheckbox vertical-filters-label']/div[@class = 'common-checkboxIndicator']\") \n",
    "price[1].click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fde8719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the Color filter to “Black”  by checking the box\n",
    "\n",
    "color = driver.find_elements(By.XPATH,\"//li[@class ='colour-listItem']//label[@class ='common-customCheckbox']/div[@class = 'common-checkboxIndicator']\") \n",
    "color[0].click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47afc9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying the url of the webpage to be scraped\n",
    "url = \"https://www.myntra.com/shoes?f=Color%3ABlack_36454f&rf=Price%3A7149.0_14099.0_7149.0%20TO%2014099.0\"\n",
    "\n",
    "# open the webpage through our web driver\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd6adc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating first 3 website pages to scrape the data\n",
    "\n",
    "page_url=[]\n",
    "\n",
    "url=driver.find_elements(By.XPATH,\"//li[@class='pagination-number']//a\")\n",
    "for i in url:\n",
    "    page_url.append(i.get_attribute('href'))\n",
    "page_url=page_url[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb09d192",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating empty list to scraping data into them\n",
    "\n",
    "brand=[]\n",
    "Description=[]\n",
    "Price=[]\n",
    "\n",
    "# extracting all the tags\n",
    "\n",
    "for i in page_url:\n",
    "    titles_brand=driver.find_elements(By.XPATH,\"//h3[@class='product-brand']\")\n",
    "    for i in titles_brand:\n",
    "        brand.append(i.text)\n",
    "    \n",
    "    titles_Description=driver.find_elements(By.XPATH,\"//h4[@class='product-product']\")\n",
    "    for i in titles_Description:\n",
    "        Description.append(i.text)\n",
    "        \n",
    "    titles_Price=driver.find_elements(By.XPATH,\"//div[@class='product-price']\")\n",
    "    for i in titles_Price:\n",
    "        Price.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5dc2f656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 sneakers listings on myntra.com:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Shoe Description</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ASICS</td>\n",
       "      <td>Men GEL-Quantum 360 6 Sports</td>\n",
       "      <td>Rs. 13999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hush Puppies</td>\n",
       "      <td>Men Solid Leather Formal Slip-Ons</td>\n",
       "      <td>Rs. 8499Rs. 9999(15% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Geox</td>\n",
       "      <td>Women Ballerinas Flats</td>\n",
       "      <td>Rs. 9891Rs. 10990(10% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fitflop</td>\n",
       "      <td>Flatform Sandals with Buckles</td>\n",
       "      <td>Rs. 7499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Geox</td>\n",
       "      <td>Men Perforations Sneakers</td>\n",
       "      <td>Rs. 12591Rs. 13990(10% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>fitflop</td>\n",
       "      <td>Embellished Leather Flatform Sandals</td>\n",
       "      <td>Rs. 7999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>fitflop</td>\n",
       "      <td>Embellished PU Comfort Pumps</td>\n",
       "      <td>Rs. 7499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>fitflop</td>\n",
       "      <td>Embellished Leather Wedge Sandals</td>\n",
       "      <td>Rs. 7499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Geox</td>\n",
       "      <td>Women Leather Slip-On Sneakers</td>\n",
       "      <td>Rs. 7693Rs. 10990(30% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Columbia</td>\n",
       "      <td>Women REDMOND V2 TrekkingShoe</td>\n",
       "      <td>Rs. 7999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Brand                      Shoe Description  \\\n",
       "0          ASICS          Men GEL-Quantum 360 6 Sports   \n",
       "1   Hush Puppies     Men Solid Leather Formal Slip-Ons   \n",
       "2           Geox                Women Ballerinas Flats   \n",
       "3        fitflop         Flatform Sandals with Buckles   \n",
       "4           Geox             Men Perforations Sneakers   \n",
       "..           ...                                   ...   \n",
       "95       fitflop  Embellished Leather Flatform Sandals   \n",
       "96       fitflop          Embellished PU Comfort Pumps   \n",
       "97       fitflop     Embellished Leather Wedge Sandals   \n",
       "98          Geox        Women Leather Slip-On Sneakers   \n",
       "99      Columbia         Women REDMOND V2 TrekkingShoe   \n",
       "\n",
       "                          Price  \n",
       "0                     Rs. 13999  \n",
       "1     Rs. 8499Rs. 9999(15% OFF)  \n",
       "2    Rs. 9891Rs. 10990(10% OFF)  \n",
       "3                      Rs. 7499  \n",
       "4   Rs. 12591Rs. 13990(10% OFF)  \n",
       "..                          ...  \n",
       "95                     Rs. 7999  \n",
       "96                     Rs. 7499  \n",
       "97                     Rs. 7499  \n",
       "98   Rs. 7693Rs. 10990(30% OFF)  \n",
       "99                     Rs. 7999  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating dataframe for the first 100 sneakers listings on myntra.com:\n",
    "\n",
    "myntra =pd.DataFrame({})\n",
    "myntra['Brand']=brand[0:100]\n",
    "myntra['Shoe Description']=Description[0:100]\n",
    "myntra['Price']=Price[0:100]\n",
    "\n",
    "print(\"Top 100 sneakers listings on myntra.com:\\n\")\n",
    "myntra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cebf1858",
   "metadata": {},
   "outputs": [],
   "source": [
    "#close the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f569e52",
   "metadata": {},
   "source": [
    "## Q8: Go to webpage https://www.amazon.in/\n",
    "Enter “Laptop” in the search field and then click the search icon.\n",
    "Then set CPU Type filter to “Intel Core i7” as shown in the below image:\n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributesfor each laptop:\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cb19dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "\n",
    "driver.get(\"https://www.amazon.in/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9299311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter “laptop” in the search field.\n",
    "search_product = driver.find_element(By.XPATH,\"//div[@class='nav-search-field ']/input\")\n",
    "search_product.send_keys(\"laptop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22b94e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click the search button\n",
    "search_btn = driver.find_element(By.XPATH,\"//div[@class='nav-search-submit nav-sprite']\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1d555d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying CPU Type filter to “Intel Core i7” filter by checking the respective box\n",
    "i7_clk=driver.find_elements(By.XPATH,\"//span[@class='a-size-base a-color-base']\")\n",
    "for i in i7_clk:\n",
    "    if i.text == 'Intel Core i7':   \n",
    "        i.click() # Checking the checkbox to apply filter on the laptop\n",
    "        break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "664ec4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty list to scraping data into them\n",
    "\n",
    "Title=[]\n",
    "Rating=[]\n",
    "Price=[]\n",
    "\n",
    "# extracting all the tags\n",
    "\n",
    "titles=driver.find_elements(By.XPATH,\"//h2[@class='a-size-mini a-spacing-none a-color-base s-line-clamp-2']\")\n",
    "for i in titles:\n",
    "    Title.append(i.text)\n",
    "    \n",
    "titles_rat=driver.find_elements(By.XPATH,\"//div[@class='a-row a-size-small']/span\")\n",
    "for i in titles_rat:\n",
    "    Rating.append(i.get_attribute(\"aria-label\"))\n",
    "        \n",
    "titles_Price=driver.find_elements(By.XPATH,\"//span[@class='a-price']\")\n",
    "for i in titles_Price:\n",
    "    Price.append(i.text[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3dcbc73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extratcing ratings from the unorganised rating tag\n",
    "\n",
    "rating=[]\n",
    "for i in range(0,len(Rating)):\n",
    "    if i == 0 or  i/2 == i//2:\n",
    "        rating.append(Rating[i][0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97f8247a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data for the first 10 laptops data on amazon.com:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ASUS TUF Gaming F15 (2021), 15.6\" (39.62 cms) ...</td>\n",
       "      <td>4.6</td>\n",
       "      <td>89,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hp Pavilion 15 12Th Gen Intel Core I7 16Gb Sdr...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>87,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lenovo ThinkBook 15 Intel 11th Gen Core i7 15....</td>\n",
       "      <td>4.3</td>\n",
       "      <td>86,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Infinix INBook X1 Pro Core i7 10th Gen - (16 G...</td>\n",
       "      <td>4.2</td>\n",
       "      <td>53,999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hp Pavilion X360 11Th Gen Intel Core I7 14 Inc...</td>\n",
       "      <td>3.7</td>\n",
       "      <td>82,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HP Envy 11th Gen Intel Evo Core i7 14 inch(35....</td>\n",
       "      <td>4.2</td>\n",
       "      <td>99,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ASUS Vivobook 15, 15.6-inch (39.62 cms) FHD, I...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>57,890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LG Gram 16 Intel Evo 11th Gen i7 Thin &amp; Light ...</td>\n",
       "      <td>3.7</td>\n",
       "      <td>85,499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HP Pavilion 14 12th Gen Intel Core i7 16GB SDR...</td>\n",
       "      <td>3.8</td>\n",
       "      <td>86,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Lenovo ThinkBook 13s Intel 11th Gen Core i7 13...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>89,990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title Rating   Price\n",
       "0  ASUS TUF Gaming F15 (2021), 15.6\" (39.62 cms) ...    4.6  89,990\n",
       "1  Hp Pavilion 15 12Th Gen Intel Core I7 16Gb Sdr...    4.0  87,900\n",
       "2  Lenovo ThinkBook 15 Intel 11th Gen Core i7 15....    4.3  86,990\n",
       "3  Infinix INBook X1 Pro Core i7 10th Gen - (16 G...    4.2  53,999\n",
       "4  Hp Pavilion X360 11Th Gen Intel Core I7 14 Inc...    3.7  82,990\n",
       "5  HP Envy 11th Gen Intel Evo Core i7 14 inch(35....    4.2  99,400\n",
       "6  ASUS Vivobook 15, 15.6-inch (39.62 cms) FHD, I...    4.3  57,890\n",
       "7  LG Gram 16 Intel Evo 11th Gen i7 Thin & Light ...    3.7  85,499\n",
       "8  HP Pavilion 14 12th Gen Intel Core i7 16GB SDR...    3.8  86,990\n",
       "9  Lenovo ThinkBook 13s Intel 11th Gen Core i7 13...    4.0  89,990"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating dataframe for the first 10 laptops data on amazon.com:\n",
    "\n",
    "amazon =pd.DataFrame({})\n",
    "amazon['Title']=Title[0:10]\n",
    "amazon['Rating']=rating[0:10]\n",
    "amazon['Price']=Price[0:10]\n",
    "\n",
    "print(\"The data for the first 10 laptops data on amazon.com:\\n\")\n",
    "amazon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8b424b",
   "metadata": {},
   "source": [
    "# Q.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec451e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Make a GET request to the website\n",
    "URL = \"https://www.azquotes.com/top_quotes\"\n",
    "page = requests.get(URL)\n",
    "\n",
    "# Parse the HTML content of the page\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "# Find all the quote elements on the page\n",
    "quote_elements = soup.find_all(\"div\", class_=\"col-6 col-sm-4 text-center margin-30px-bottom sm-margin-30px-top\")\n",
    "\n",
    "# Initialize a list to store the scraped data\n",
    "quotes = []\n",
    "\n",
    "# Loop through each quote element\n",
    "for quote_element in quote_elements:\n",
    "    # Extract the quote text\n",
    "    quote = quote_element.find(\"a\", class_=\"theme-blue-hover\").text.strip()\n",
    "\n",
    "    # Extract the author name\n",
    "    author = quote_element.find(\"a\", class_=\"theme-grey-hover\").text.strip()\n",
    "\n",
    "    # Extract the type of quote\n",
    "    quote_type = quote_element.find(\"small\", class_=\"theme-grey\").text.strip()\n",
    "\n",
    "    # Append the scraped data to the quotes list\n",
    "    quotes.append({\"quote\": quote, \"author\": author, \"quote_type\": quote_type})\n",
    "\n",
    "# Print the first 10 quotes\n",
    "for quote in quotes[:10]:\n",
    "    print(f\"{quote['quote']} - {quote['author']} ({quote['quote_type']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6fc182",
   "metadata": {},
   "source": [
    "# Q.10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f629134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Make a GET request to the website\n",
    "URL = \"https://www.jagranjosh.com/general-Knowledge/List-of-all-Prime-Ministers-of-India-142485\"\n",
    "page = requests.get(URL)\n",
    "\n",
    "# Parse the HTML content of the page\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "# Find the table containing the information\n",
    "table = soup.find(\"table\")\n",
    "\n",
    "# Find all the rows in the table\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "# Initialize a list to store the scraped data\n",
    "prime_ministers = []\n",
    "\n",
    "# Loop through each row in the table\n",
    "for row in rows[1:]:\n",
    "    # Find all the cells in the row\n",
    "    cells = row.find_all(\"td\")\n",
    "\n",
    "    # Extract the information from the cells\n",
    "    name = cells[0].text.strip()\n",
    "    born_dead = cells[1].text.strip()\n",
    "    term_of_office = cells[2].text.strip()\n",
    "    remarks = cells[3].text.strip()\n",
    "\n",
    "    # Append the scraped data to the prime_ministers list\n",
    "    prime_ministers.append({\"Name\": name, \"Born-Dead\": born_dead, \"Term of office\": term_of_office, \"Remarks\": remarks})\n",
    "\n",
    "# Create a Pandas dataframe from the prime_ministers list\n",
    "df = pd.DataFrame(prime_ministers)\n",
    "\n",
    "# Display the first 5 rows of the dataframe\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13f70ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
